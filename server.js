import express from "express";
// import OpenAI from "openai";
import dotenv from "dotenv";

dotenv.config();

const router = express.Router();

// // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ OpenAI ÐºÐ»Ð¸ÐµÐ½Ñ‚
// const openai = new OpenAI({
//   apiKey: process.env.OPENAI_API_KEY,
// });

// // Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ retry Ð¿Ñ€Ð¸ 429
// async function createChatCompletion(message, retries = 3) {
//   try {
//     const completion = await openai.chat.completions.create({
//       model: "gpt-3.5-turbo",
//       messages: [{ role: "user", content: message }],
//     });

//     const aiResponse = completion.choices[0]?.message?.content;
//     if (!aiResponse) throw new Error("AI returned empty response");

//     return aiResponse;
//   } catch (err) {
//     // Ð•ÑÐ»Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½
//     if (err.status === 429 && retries > 0) {
//       console.warn("â³ 429 Too Many Requests, retrying in 2s...");
//       await new Promise((resolve) => setTimeout(resolve, 2000));
//       return createChatCompletion(message, retries - 1);
//     }
//     throw err;
//   }
// }

router.post("/", async (req, res) => {
  const { message } = req.body;

  if (!message) return res.status(400).json({ error: "Message is required" });

  try {
    // const aiResponse = await createChatCompletion(message);
    // res.json({ response: aiResponse });

    // Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐµÑ€Ð²ÐµÑ€ Ð½Ðµ Ð¿Ð°Ð´Ð°Ð»
    process.on('uncaughtException', (err) => console.error('Uncaught Exception:', err));
process.on('unhandledRejection', (reason, promise) => console.error('Unhandled Rejection at:', promise, 'reason:', reason));
    res.json({ response: "âœ… Server is working, OpenAI calls are disabled for now." });
  } catch (err) {
    console.error("ðŸ”¥ OpenAI API error:", err);
    res.status(500).json({ error: "AI chat failed" });
  }
});

export default router;
